%%%% 1. DOCUMENTCLASS %%%%
\documentclass[journal=tosc,final]{iacrtrans}
%%%% NOTES:
% - Change "journal=tosc" to "journal=tches" if needed
% - Change "submission" to "final" for final version
% - Add "spthm" for LNCS-like theorems


%%%% 2. PACKAGES %%%%
\usepackage[left, pagewise,edtable]{lineno}
\usepackage{blt}
\usepackage{graphicx}
\usepackage{framed} 
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{xcolor} 
\colorlet{shadecolor}{gray!25}
\definecolor{mshadecolor}{rgb}{0.7421875,0.7421875,0.7421875}
\setlength{\OuterFrameSep}{10pt}
%%%% 3. AUTHOR, INSTITUTE %%%%
\author{Moritz Rupp}
\institute{
  Hochschule Albstadt-Sigmaringen, Albstadt, Germany, \email{ruppmori@hs-albsig.de}
  
}
%%%% NOTES:
% - We need a city name for indexation purpose, even if it is redundant
%   (eg: University of Atlantis, Atlantis, Atlantis)
% - \inst{} can be omitted if there is a single institute,
%   or exactly one institute per author


%%%% 4. TITLE %%%%
\title{Fuzzing Methods}

\author{Moritz Rupp}
%%%% NOTES:
% - If the title is too long, or includes special macro, please
%   provide a "running title" as optional argument: \title[Short]{Long}
% - You can provide an optional subtitle with \subtitle.

\begin{document}

\maketitle
\author


%%% 5. KEYWORDS %%%%
\keywords{Offensives Security \and Fuzzing \and Brute-Forcing \and It-Security \and Testing }


%%%% 6. ABSTRACT %%%%
\begin{abstract}
Modern Software has become increasingly more complex. This is connected with growing numbers of security bugs and cyberattacks. Software testing tries to counter that development. Fuzzing is a dynamic automated way of testing systems and sees growing usage among security profesionals. It has gained growing attention after several zero day vulnerabilitys have been found with this approach. This paper examines different methods to use this technology. 
\end{abstract}

%%%% 7. PAPER CONTENT %%%%
\section{Introduction}
Development of any kind has always come with bugs, errors and unintentionally behaviour. Software is no exception. Increasingly complex programs and growing technology stacks contribute to this problem. On top of that, finished components are often continuously integrated. As a result, it becomes ever more difficult to predict final program behaviour. This yields in risks, particular if an application is processing external data input. If not handled correctly it can pose security vulnerabilitys or data breaches.

The bigger a programm gets, more often such unwanted appearances occur. A field in which this is most noticable is web-development. Whereas 15 years ago, most websites were built on top of a view technologys, modern web-applications often use several framework with huge amounts of dependencys. This lead to an increasing amount of security bugs[2]. Different approaches in software testing tries to oppose that development.

Back when software was relativly simple and free of dependencys, manuel testing was the state of play. That included code reviews and manuel checks for potentiel exploitation. This was time consuming and needed experts for every specific application. Therefore static analyis was quickly adapted. This contained new techniques such as pattern search with a control flow graph, data dependency graph and data flow analysis[3]. When software grew even more in complexity, security researchers needed a more scalable approach. Hence dynamic analyis was implemented. This method tries to understand software behaviour by testing and evaluating the target system while it is running. 

Fuzz-testing is the cutting edge of this technique and will be explored in this paper. A particular emphasis will be led on the different methods used in fuzzing. 
At first we will examine the basic concepts and functionality of the general process. On top of that different methods in Fuzzing will be explored. That expresses itself in how certain functionalitys of the fuzzing application are executed. This section will be the main focus of the paper. Following we will also cover practical usage by showcasing different tools and a concrete example. 

As a final point we will have a conclusion in which we provide a summary and an assessment of the pros and cons of Fuzz-testing. 
\newpage
\section{Fuzzing background}
Fuzz testing or fuzzing is a method of testing software to detect security holes in applications, operating systems, and networks. It involves flooding their input interfaces with random data, called fuzz, to make them crash or trigger unexpected programm behaviour. Furthermore the computation results are monitered and reported, all in an automated way. Fuzzing has become increasingly more relevant among security researchers and is used by almost all big tech companys for penetration testing[4]. Tech leaders find more than 80 of their bugs by using different fuzzing Methods[5]. Google for example  has found over 15 thousend errors in their browser projekt chrome[6]. Microsoft used dynamic analysis to test one of their flagship projects 'office' and found nearly 2000 bugs using the fuzzing tool afl[7]. Also open source project such as linux use Fuzzing methods on a great scale. Many security bugs within the Linux Kernel have been found this way[8].\\
Generally a distinction can be made on how fuzzing is executed on a specific interface. Depending on the application or infrastructure of the target system, different types of fuzzers can be used. Application Fuzzing will approach a system by testing functionalitys such as buttons, input fields or command line parameters. File format fuzzing tries to generate corrupted files and feed them to the corresponding target for processing. Other types of fuzzing targets include Protocol fuzzing and network fuzzing.\\
In addition more distinction can be made on how the fuzzing inputs or test cases are generated. There are two approaches. Mutation based Fuzzers alter existing data samples to create new test cases.
Generation based fuzzers will create test cases from scratch based on protocoll knowledge. 
Both methods have pro and cons and will be covered in more detail later on.

\section{Functionality}
This section will provide a perspective on the basic concept of Fuzzing.

Regardless of the specific implementation, the core process of all fuzzers remains the same. First the target interface has to be identified. Relevant are all interfaces that allow external data input. This ranges from network sockets, user-input on a website, to linux binarys et cetera. Specifically interesting are interfaces that are reachable by unpriviliged users. The act of finding those target interfaces highly depends on the tested application and could include things like code reviews or portscanning. 


Based on the findings, the fuzzer input can now be generated. This is one main task of the actual fuzz-engine. In the most basic case, it will generate pseudo random inputs based of the given seed. The seed could contain anything that makes sense in the context of the expected data[9]. Essentially a snapshoot of valid inputs for the target system. This will be mostly combinations of strings and intergers. 

Following the test cases can be passed to the  target system. Depending on the identfied interfaces, different transfer methods are used. This could involve HTTP Requests, tcp sockets or simply files or input parameters for a programm. While the Fuzz-engine is sending the generated inputs, the target system has to be monitered to track how the testcases effect the system behaviour. Monitering and tracking can be achieved with different approaches. A convinient but not always available case are logging functions of the target system. They can be simply read out during or after the fuzzing process. If the target is lacking such functionalitys they first have to be implemented. 

More features are feedback instrumentations. Those are compile time binarys that are inserted into the target code to track and tell the fuzzer which parts of the internal programmflow is effected and has already been covered. This ensures to establish a correlation between test cases and programm behaviour. The Feedback gets send back to the fuzzing-engine and can be read out or reprocessed[10].
\newpage

\noindent Figure 1 provides an overview of a basic mutation based fuzzing implementation. The Fuzzer as seen between the Seed and the target system includes a mutation engine and genetic algorithms. The Fuzzing process can now lapse as follows.\\
The Seed streams valid inputs to the fuzzer that will modifiy these with the help of the mutation engine. Following the mutated inputs gets send to the target system in which feedback instrumentations listen for exceptions, log them and provide feedback to the fuzzer. Based on that genetic algorithms produce further inputs.

Once the fuzzing process has endet, it is now possible to analyse and minimize the findings. This includes to determine if a tracked exeption could actually cause a vulnability or even an exploit. Furthermore these assessments are rated. This is done by ranking frameworks like the common vulnability scoring system(CVSS)[11]. Finally all relevant informations are summed up, reported and passed to the corresponding responsible. 
\begin{figure}
\caption{Basic Fuzzing application}
 \begin{shaded}

\begin{center}
\includegraphics[scale=0.2]{../final2.png}
\end{center}
\end{shaded}

\end{figure}

\section{Fuzzing Methods}
Due to the huge landscape of applications, infrastructures and use cases, enourmes classifications exist around fuzzing. 

In this section we will further examine the different methods used to generate the fuzzing input, like random, mutation and generation based fuzzing. Afterwards we will also cover different types of target focused fuzzers such as application or file format fuzzing et cetera. 
\subsection{Random fuzzing}
When fuzzing was invented in 1988 by Professor Barton P. Miller at the University of wisconsin, random fuzzing was the very first approach[12].

When Miller held a lecture remotely via a telephone connection, a thunderstorm caused electrical disturbances in the telephoneline. This resulted in garbled transmissions. Based on these random occuranced, Miller stated that bad input  could be used to crash other systems. He and his students applied this observation on different unix utilites their where using at that time. They remarked that  more than 30\% of the tested programms where vulnerable to bad input[13]. 

Initally they simply manuelly passed pseudo random inputs to these programms. Later on miller made a programming assigment to his students, that stated to automate this task. The very first fuzzer was born. The basic principle still holds up today. 

Random fuzzers will simply generate inputs by randomly concatenate characters that are provided in a seed. The method is mainly used for black box testing, since in there input formats are mostly unknown. A clear downside are the huge amounts of invalid inputs such a fuzzer will produce. 

Most of the generated test cases are getting rejected by the parser of the target programm. Hence random fuzzers are often used to test parser applications.

Another usecase are to fuzz strings of programm parameters. They barely follow any specific structure and are therefore an ideal target for random bazzed fuzzers. 
\subsection{Mutation based fuzzing}
As we already know, this method will simply alter already existing data. This can be based of available information in case the target inputs are known. Otherwise it is first neccecarry to record valid inputs. This can be achieved via proxys that will record communication or interaction patterns. These valid inputs can then be mutated, either in a completly random manner or after fixed patterns. Figure 2 provides a mutation based implementation in pseudo code. 

\begin{figure}[h]
\caption{Mutated fuzzing outputs}
 \begin{lstlisting}[language=python,style=code]
	seed = "https://www.hs-albsig.de/such?tx\_solr[q]"
	Fuzzer = MutationFuzzer(seed=seed)
	Fuzzer.mutate for i in range(5) -->
	https://wGw.h2-albsig.re/sLh?tx\_soslr[qx=?]
	htSpK://wwl.hs-albsisag.de/such?tx\_solr[s]a
	htSps://o?w.h_s-lbsig.de/such?tx\_solr[q]=\p
	h?tpIl//wsw.hs-albswgqde/such?tx\_solr[q]=q\
	https://www.hs-albsig.de/such?tx\_solr[q]=\\
\end{lstlisting}

\end{figure}
At first, in line 1 the seed is determined. Following a Fuzzer object is applied and the seed is passed(see line 2). Then we call the fuzzer method to mutate the given inputs without setting up any fixed patterns. This results in several mutated urls. The inner procedure is simple. The fuzzer engine will take a random index of the seed in the range of the string length. The index value will then be flipped.  Here we instantly see why mutation based fuzzers are often called 'dumb fuzzers'. 
Most of the generated outputs are invalid urls, and will be rejected by the parser of the target system. In fact several of the generated urls will never reach our inital target, the webserver. This implementation will moreover fuzz the url-parser than the http-server.
Therefore it is neccecarry to be very patience about which part of valid inputs should get passed to the seed to be fuzzed. 

In this concrette example the right approach would be to setup a  static corpus and only mutate the string of the file path and parameters. 
These can then be combined and send out.


\begin{figure}[h]
 \caption{Right approach}
 \begin{lstlisting}[style=code]
	static = https://www.hs-albsig.de/  
	seed = suche/?tx\_solr[q]=\\
 \end{lstlisting}
\end{figure}


Even more precision can be achived by providing multiple seeds that represent different parts of the valid format. A simple function can then receive mutation of those seeds and concatenate them within the expected format.

Still mutation based fuzzers lack in efficiency and strongly depend on the right configuration. 
\newpage
\subsection{Generation based fuzzing}
In Generation-Based fuzzing new data is defined based on the expected input of the target specification. Hence knowledge of the system under test is mandatory. It is neccecarry to have a well understanding of the input format, since test cases are generated completly from scratch. It's also importand since we want to ensure to pass the parser of the target system, so our test cases are actually reaching the application.
Generation based fuzzers achieve this with the help of formal languages. Those are essentially a set of strings of a predefined alphabet. With grammars it is possible to form different expressions based of that. A grammar describes how to form valid strings from the language's alphabet. This is achieved by assigning rules to individual parts of it.
In most cases so called context free grammars are used. With them it is possible to form lots of relevant languages. They are particularly great for expressing syntactical structures. This can range from actuall programming languages to formats such as JSON or standardized naming conventions like urls. Figure 4 provides a context free grammar to generate valid urls.
\begin{figure}[h]
 \caption{URL Grammar}
 \begin{center}
\begin{shaded}
\begin{internallinenumbers}
\begin{verbatim}
"<start>":
     ["<url>"],
"<url>":
     ["<scheme>://<authority><path><query>"],
"<scheme>":
     ["http", "https", "ftp", "ftps"],
"<authority>":
     ["<host>", "<host>:<port>", "<userinfo>@<host>"]
"<host>":  
     ["hs-albsig.de", "www.albstadt.de", "github.com"],
"<port>":
     ["80", "443", "<nat>"],
"<nat>":
     ["<digit>", "<digit><digit>"],
"<digit>":
     ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"],
"<userinfo>":  
     ["user:password"],
"<path>":  
     ["", "/", "/<id>"],
"<id>":  
     ["abc", "def", "x<digit><digit>"],
"<query>":
     ["", "?<params>"],
\end{verbatim}
\end{internallinenumbers}
\end{shaded}
\end{center}

\end{figure}

A grammar always begins with a start-symbol, followed by a set of expansion rules, which indicates how symbols can be expanded or built. 
Essentially we sequentially run over expressions and apply expension rules on the symbols. This means that the symbol on the left, can be replaced by the string on the right. In our example in Figure 4, we would replace the start symbol in line 1 with the string in line 2. This string would then be built with the further symbols and strings continuously. When fully traversed, the grammar is capable of generating various valid urls.
\begin{figure}[h]
 \caption{Grammar generated urls}
 \begin{lstlisting}[style=code]
	http://hs-albsig.de?abc=x23
	ftp://user:password@www.albstadt.de=?def&x23
	https://github.com
	ftps://user:password@hs-albsig.de?=def
	http://www.albstadt.de/x68
 \end{lstlisting} 
\end{figure}
\newpage
We can see that all generated urls are within an accaptable format of a web-parser, hence will actually reach our target system.

Fuzzing tools that offer generation based funktionalities use such grammars in their inner working flow to create test-cases. In reality it is not neccecarry to setup them yourself. Generation based tools instead require to split up input data into its individual parts and then assign rules to them. These creation rules determine how to handle and concatenate every single part in order to built up a valid test-case. The following example shows the first part of an http request, built with such creation rules of the fuzzing tool Sully.
\begin{figure}[h]
 \caption{Creation rules in Sully}
 \begin{lstlisting}[style=code]
	s_static('GET')
	s_delim(" ")
	s_string('index.html ')
	s_string('HTTP')
 \end{lstlisting}
\end{figure}
The individual parts of an http request are passed to the corresponding rules. Since the protocoll follows a strict syntax via http verbs, the 'GET' string in line 1 is set to static. Every part is continuously assigned to rules that ensure are within the protocolls format. Only Parameters such as strings are set up to be fuzzable.
   
The clear downside of this method is the big configuration needed. It is neccecarry to setup the fuzzer accordingly for every system from scratch. 
\subsection{Fuzzer types}
As we have seen already, it is crucial to choose the right fuzzing method. Otherwise we might fail to reach the target system with our test cases. Thats why further classifications can be made on the different types of fuzzers. They determine the attack vectors of the system under test. 

Application fuzzers will always focus on the I/O of a system. This includes the user interface, command line parameters or import/export capabilities. For a web application this also means urls, forms or RPC requests. With the help of proxys and sniffers it is fairly simple to record valid inputs of those vectors, hence mutation based fuzzing is often choosen as a method. Application fuzzing  is also among the most used fuzzer type since it sees the most amount of user traffic.

Protocol fuzzers will mostly focus on communication patterns. This could be different data packets of network traffic like http requests, tcp streams et cetera. Since such protocolls are often complex and very case sensitive, it requires good understanding of their inner working flow in order to test them. Hence generation based fuzzers are the method of choice.

File format fuzzing will fuzz different file formats such as jpg or png. This is achieved by generating multiple malformed samples which are then opened in the target application. 
If the application crashes, the file is saved for later review. 

The main goal is then to gain control over the crash. This might led to an exploit.

Since users are more likely to interact with images than executables it is espacially dangerous if findings occur in such file types.
\section{Tooling}
Since Fuzzing required lots of preconfiguration and setup, it is almost always executed with the help of different tools. Only in rare occations it would make sense to manually fuzz a system. This specifically includes old software that is not compatible with modern fuzzing tools. Otherwise all systems that offer interfaces and compute external input, are fuzzable with most modern tools. 

Such fuzzing tools will mainly provide 3 things. Generate test cases as inputs for the system under test, built a connection channel to transfer those inputs to the target interface and monitor the system behaviour.

There are dozens of commerial fuzzing engines and hundrets of self made implementations. Big tech companys often built their own fuzzers to test their it infrastructures. Googles `OSS-Fuzz` was able to find tousends of bugs in many different google applications. 

Among the most relevant fuzzing tool is the american fuzzing loop(afl).
Afl is responsible for a majority of zero day vulnability findings and will be covered in the folowing example.
\subsection{Practical example}
The american fuzzing loop is a powerfull open-source fuzzer that offers all of the covered methods and techniques. On top of that it employs genetic algorithms and feedback instrumentations to increase code coverage. In the following example we will fuzz the C programm Fuzzgoat. It has several deliberate memory corruption bugs that are easily found by afl. Bevore the  fuzzing process can start, we have to do some preconfigurations. 
\begin{figure}[h]
 \caption{afl configuration}
 \begin{lstlisting}[style=code]
    $~ export CC=afl-clang-fast
    $~ make
    $~ mkdir afl_in
    $~ cp /bin/ps afl_in/   
    $~ mkdir afl_out
    $~ afl-fuzz -i afl_in -o afl_out -- ./fuzzgoat @@
\end{lstlisting}
\end{figure}

At first it is neccecarry to compile the the target binary with afl's built in compiler. This way the target is setup for feedback instrumentation and afl is able to read out code paths and general code coverage.

In line 1 we tell afl to use the compiler afl-clang-fast, to then compile the binary with it.
Following we create a directory afl\_in. In line 4 we copy the binary of an Unix utility to this directory. This is essentially the seed of which afl will create the test cases. To store results we create another directory afl\_out. In there afl will provide information about crashes and bugs. 
In line 6 we start the actual fuzzing process by passing the seed and output locations as well as the target binary. 

Afl will now take inputs of the seed, mutate them and insert the test cases in the target applicationâ€™s command structure.
When afl is stared, it spawns an interface in the terminal that provides different informations about the fuzzing process. This includes the amount of code coverage, which test cases are currently passed and the number of findings. 
\begin{figure}[h]
 \caption{AFL fuzzing}
 \begin{center}
   \includegraphics[scale=0.3]{afl.png}

 \end{center}
\end{figure}
\newpage
Depending on the tested application it can be neccecarry to run afl for several hours to days. When finished it is possible to examine the findings in the output folder. In there afl lists all the test cases that posed crashes and bugs. Following those findings can then be analysed and rated.

As a final act reports can be written.


\section{Conclusion}
Fuzzing is very convinient and efficient, yet depends on the right configuration and is not easy to setup.
\section{References}
[1] \textbf{Aimee O'Driscoll}(2022), 25+ cyber security vulnerability statistics and facts of 2022, https://www.comparitech.com/blog/information-security/cybersecurity-vulnerability-statistics/\\
\\
\noindent[2] \textbf{Emilio Granado Franco}(2022),  Global Risks Report 2022, Chapter 3. Digital Dependencies and Cyber Vulnerabilities\\

\noindent[3] \textbf{Chess, B. and McGraw, G.}(2004), Static analysis for security, IEEE Security \& Privacy, 10.1109/MSP.2004.111
\\


\noindent[4] \textbf{Li, Jun and Zhao, Bodong and Zhang, Chao}(2018),     Fuzzing: a survey, SpringerOpen
\\

\noindent [5] \textbf{Sergej Dechand}(2020), Introduction and fuzzing 101 @Fuzzconn Europe 2020, CodeIntelligence, https://www.youtube.com/watch?v=7AdzrXbUew0
\\

\noindent [6] \textbf{Liam Tung}(2019),  Google: We've open-sourced ClusterFuzz tool that found 16,000 bugs in Chrome, https://www.zdnet.com/article/google-weve-open-sourced-clusterfuzz-tool-that-found-16000-bugs-in-chrome/
\\

\noindent [7] \textbf{Sergej Dechand}(2020), Introduction and fuzzing 101 @Fuzzconn Europe 2020, CodeIntelligence, https://www.youtube.com/watch?v=7AdzrXbUew0
\\

\noindent[8] \textbf{Jonathan Reimer}(2020), 5 CVEs Found With Feedback-Based Fuzzing, https://www.code-intelligence.com/blog/5-cves-found-with-feedback-based-fuzzing
\\

\noindent[9] \textbf{Li, Jun and Zhao, Bodong and Zhang, Chao}(2018),     Fuzzing: a survey, SpringerOpen
\\


\noindent[10] \textbf{Cornelius Aschermann}(2020), Algorithmic Improvements for
Feedback-Driven Fuzzing, SpringerOpen
\\

\noindent[11] \textbf{NIST}(2019), Vulnerability Metrics, https://nvd.nist.gov/vuln-metrics/cvss
\\

\noindent[12] \textbf{Parul Garg}(2012), Fuzzing: Mutation vs. generation, https://resources.infosecinstitute.com/topic/fuzzing-mutation-vs-generation/
\\

\noindent[13] \textbf{Andreas Zeller}(2022), Fuzzing: Breaking things with random inputs, https://www.youtube.com/watch?v=YjO1pIx7wS4/
\\
\end{document}
